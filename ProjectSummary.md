Since the data I needed was contained in a single table on the Falcon 9 launch history page, I didn't require the full power and complexity of Scrapy, so I decided to use Beautiful Soup instead of Scrapy for this project because it is easier to use and more beginner-friendly for small-scale web scraping tasks, allows for more manual control when working with simple HTML pages, and allowed me to quickly get started and efficiently gather the data despite requiring setting up an entire scraping structure.
The initial phase of the project entailed comprehending the website's architecture and locating the relevant data. After looking over the website, I discovered the launch table, which had information on the booster version, launch date, and launch location. I then created the scraping code to collect only the launches that used boosters from Blocks 4 or 5.  After isolating the table rows, I retrieved and filtered pertinent columns, such as the location, launch date, and booster name. The most significant time-saver was being able to pinpoint which rows and columns to remove, which improved the efficiency and focus of the scraping operation.
I created a prompt while using ScrapeGraphAI that requested that the program locate Falcon 9 launches especially for Block 4 and Block 5 boosters and provide the booster name, launch date, location, and, if possible, the prior reuse date. The prompt's objective was to produce an organized output that reflected the crucial information required for the study. The request was too general at first, and the results weren't very useful, but I was able to extract more precise information after I narrowed it down to be more particular. I discovered that when using programs like ScrapeGraphAI, giving precise directions produces far better results, particularly when working with structured data.
I mostly used Jupyter Notebook to design and structure the script for this assignment. When I required to quickly test or print portions of the code to comprehend what was occurring. When attempting to troubleshoot data problems or validate the way turnaround times were computed, Jupyter proved to be quite useful. The entire process involved setting up the entire code in Jupyter Notebook after testing smaller portions. If importing some libraries had a problem I used visual code.  
In order to create the program, I divided the work into manageable chunks. I started by concentrating on gathering and purifying the data. After that, I compared the launch dates to determine which booster had the quickest turnaround time. I then concentrated on figuring out which booster had the most launches. To keep things structured and prevent myself from being overwhelmed, I took each of these actions one at a time. Managing empty data lists when no pertinent boosters were discovered and ensuring that date formats were constant so I could precisely determine the number of days between launches were some of the challenges I encountered. In addition, I had to check for logical mistakes, such as evaluating the same launch again. In spite of these obstacles, I managed to properly execute the code and provide the necessary output to both text files.
